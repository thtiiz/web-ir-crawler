{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "colab": {
      "name": "multithread-webcrawler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfLrZkc5wdIU"
      },
      "source": [
        "import re\n",
        "import argparse\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "\n",
        "import os, codecs\n",
        "from datetime import datetime \n",
        "from urllib.robotparser import RobotFileParser\n",
        "from urllib.parse import urljoin, urlparse, unquote, urlsplit, urlunsplit\n",
        "from requests.exceptions import HTTPError\n",
        "from queue import Queue\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import ssl\n",
        "import threading\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVbvy18vpSXI"
      },
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "class Transcript(object):\n",
        "    def __init__(self, filename):\n",
        "        self.terminal = sys.stdout\n",
        "        self.logfile = open(filename, \"a\")\n",
        "\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.logfile.write(message)\n",
        "\n",
        "    def flush(self):\n",
        "        # this flush method is needed for python 3 compatibility.\n",
        "        # this handles the flush command by doing nothing.\n",
        "        # you might want to specify some extra behavior here.\n",
        "        pass\n",
        "\n",
        "\n",
        "def startTC(filename):\n",
        "    \"\"\"Start transcript, appending print output to given filename\"\"\"\n",
        "    sys.stdout = Transcript(filename)\n",
        "\n",
        "\n",
        "def stopTC():\n",
        "    \"\"\"Stop transcript and return print functionality to normal\"\"\"\n",
        "    sys.stdout.logfile.close()\n",
        "    sys.stdout = sys.stdout.terminal\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d1oUo49wdIZ"
      },
      "source": [
        "def remove_query_from_url(url):\n",
        "    parsed = urlparse(url)\n",
        "    return \"\".join([parsed.scheme,\"://\",parsed.netloc,parsed.path])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBqHSVPSF-bY"
      },
      "source": [
        "def requests_retry_session(\n",
        "    retries=10,\n",
        "    backoff_factor=0.3,\n",
        "    status_forcelist=(500, 502, 504),\n",
        "    session=None,\n",
        "):\n",
        "    session = session or requests.Session()\n",
        "    retry = Retry(\n",
        "        total=retries,\n",
        "        read=retries,\n",
        "        connect=retries,\n",
        "        backoff_factor=backoff_factor,\n",
        "        status_forcelist=status_forcelist,\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry)\n",
        "    session.mount('http://', adapter)\n",
        "    session.mount('https://', adapter)\n",
        "    return session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjwNk1IOwdIa"
      },
      "source": [
        "class Crawler(threading.Thread):\n",
        "    current_i = 0\n",
        "\n",
        "    def __init__(self, sleep_time, seed_url, max_num, whitelist_file_types, user_agent, whitelist_domain, frontier_q, visited, url_lock, parsed_sitemap_domains, parsed_robots_domains, num_lock):\n",
        "        threading.Thread.__init__(self)\n",
        "        print(f\"Web Crawler worker {threading.current_thread()} has Started\")\n",
        "        self.sleep_time = sleep_time\n",
        "        self.seed_url = urljoin(seed_url, '/')[:-1]\n",
        "        self.visited = visited\n",
        "        self.url_lock = url_lock\n",
        "        self.num_lock = num_lock\n",
        "        self.frontier_q = frontier_q\n",
        "        self.max_num = max_num\n",
        "        self.whitelist_file_types = whitelist_file_types\n",
        "        self.user_agent = user_agent\n",
        "        self.whitelist_domain = whitelist_domain\n",
        "        self.parsed_robots_domains = parsed_robots_domains\n",
        "        self.parsed_sitemap_domains = parsed_sitemap_domains\n",
        "        self.headers = {\n",
        "            'User-Agent':  user_agent,\n",
        "            'From': 'thitiwat.tha@ku.th'\n",
        "        }\n",
        "\n",
        "    def link_parser(self, raw_html):\n",
        "        soup = BeautifulSoup(raw_html, 'html.parser')\n",
        "        urls = []\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            urls.append(link.get('href'))\n",
        "        # pattern = '<a href=\"([^\"]*)\"'\n",
        "        # urls = re.findall(pattern, raw_html)\n",
        "        return urls\n",
        "\n",
        "    def de_duplicate_urls(self, urls):\n",
        "        return list(set(urls))\n",
        "    \n",
        "    def get_base_url(self, url):\n",
        "        return urljoin(url, '/')[:-1]\n",
        "\n",
        "    def save_to_disk(self, url, raw_html):\n",
        "        try:\n",
        "            parsed = urlparse(url)\n",
        "            hostname = parsed.hostname\n",
        "            url_path = parsed.path\n",
        "\n",
        "            # print(f'hostname {hostname}')\n",
        "            # save_folder_path = 'html/' + hostname + url_path\n",
        "            # save_filename = 'index.html'\n",
        "            filetype = re.match(r'.*\\.(.*)$', url_path)\n",
        "\n",
        "            if(filetype != None):\n",
        "                # urljoin 'http://localhost:8888/asdasd/htasd.html' => 'http://localhost:8888/asdasd/'\n",
        "                save_folder_path = 'html/' + hostname + \"/\".join(url_path.split('/')[:-1])\n",
        "                save_filename = url.split(urljoin(url, '.'))[1]\n",
        "            else:\n",
        "                save_folder_path = 'html/' + hostname + url_path\n",
        "                save_filename = 'index.html'\n",
        "            \n",
        "            save_folder_path = save_folder_path.strip('/')\n",
        "            save_abs_path = save_folder_path + '/' + save_filename\n",
        "\n",
        "            print(f'savepath: {save_folder_path}')\n",
        "            print(f'save filename: {save_filename}')\n",
        "            print(f'save_abs_path: {save_abs_path}')\n",
        "\n",
        "            os.makedirs(save_folder_path, 0o755, exist_ok=True)\n",
        "            f = codecs.open(save_abs_path, 'w', 'utf-8')\n",
        "            f.write(raw_html)\n",
        "            f.close()\n",
        "        except:\n",
        "            print(f'Error in save_to_disk')\n",
        "\n",
        "    def normalization_urls(self, urls, base_url):\n",
        "        try:\n",
        "            # absolute\n",
        "            urls = [urljoin(base_url, url) for url in urls]\n",
        "\n",
        "            # remove # (self reference)\n",
        "            urls = [re.sub(r'#.*', '', url) for url in urls]\n",
        "\n",
        "            # parse to utf8\n",
        "            urls = [unquote(url) for url in urls]\n",
        "\n",
        "            # strip / (backslash)\n",
        "            urls = [url.strip('/') for url in urls]\n",
        "\n",
        "            # remove query string\n",
        "            urls = [remove_query_from_url(url) for url in urls]\n",
        "\n",
        "            return urls\n",
        "        except:\n",
        "            print(f'Error in normalization_urls')\n",
        "\n",
        "    def get_raw_html(self, url):\n",
        "        text = ''\n",
        "        try:\n",
        "            response = requests_retry_session().get(url, headers=self.headers, timeout=30, verify=False)\n",
        "            # If the response was successful, no Exception will be raised\n",
        "            response.raise_for_status()\n",
        "        except HTTPError as http_err:\n",
        "            print(f'HTTP error occurred: {http_err}')  # Python 3.6\n",
        "        except Exception as err:\n",
        "            print(f'Other error occurred: {err}')  # Python 3.6\n",
        "        else:\n",
        "            # print('Success!')\n",
        "            text = response.text\n",
        "        finally:\n",
        "            return text\n",
        "\n",
        "    def check_and_save_robots(self, robot_url):\n",
        "        try:\n",
        "            raw = self.get_raw_html(robot_url)\n",
        "            is_valid = 'User-agent' in raw\n",
        "            if(is_valid):\n",
        "                self.save_to_disk(robot_url, raw)\n",
        "            return is_valid\n",
        "        except:\n",
        "            print(f'Error in check_and_save_robots')\n",
        "            return False\n",
        "\n",
        "    def get_parsed_robots(self, base_url):\n",
        "        rp = RobotFileParser()\n",
        "        robots_url = base_url + '/robots.txt'\n",
        "        try:\n",
        "            is_valid = self.check_and_save_robots(robots_url)\n",
        "            if(not is_valid):\n",
        "                raise Exception(\"Not found robots\")\n",
        "\n",
        "            if(base_url not in self.parsed_robots_domains):\n",
        "                self.parsed_robots_domains.add(base_url)\n",
        "            rp.set_url(robots_url)\n",
        "            rp.read()\n",
        "            \n",
        "        except:\n",
        "            # allow all\n",
        "            rp.set_url('https://ku.ac.th')\n",
        "            rp.read()\n",
        "        finally:\n",
        "            return rp\n",
        "\n",
        "\n",
        "    def filters_urls(self, urls, base_url):\n",
        "        filtered_urls = []\n",
        "        rp = self.get_parsed_robots(base_url)\n",
        "        for url in urls:\n",
        "            parsed = urlparse(url)\n",
        "            url_path = parsed.path\n",
        "            hostname = parsed.hostname\n",
        "            # check domain allow only ku.ac.th\n",
        "            if(not hostname or self.whitelist_domain not in hostname):\n",
        "                continue\n",
        "\n",
        "            # check can fetch from robots.txt\n",
        "            can_fetch = rp.can_fetch(self.user_agent, url)\n",
        "            if(not can_fetch):\n",
        "                continue\n",
        "\n",
        "            # check filetype\n",
        "            filetype = re.match(r'.*\\.(.*)$', url_path)\n",
        "\n",
        "            if(not filetype):\n",
        "                filtered_urls.append(url)\n",
        "            elif(filetype[1] in self.whitelist_file_types):\n",
        "                filtered_urls.append(url)\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "        return filtered_urls\n",
        "\n",
        "    def include_urls(self, urls, base_url):\n",
        "        try:\n",
        "            if(base_url in self.parsed_sitemap_domains):\n",
        "                return urls\n",
        "\n",
        "            xml = self.get_raw_html(base_url + '/sitemap.xml')\n",
        "            soup = BeautifulSoup(xml)\n",
        "            urlsetTag = soup.find_all(\"loc\")\n",
        "            urlsetTag = list(urlsetTag)\n",
        "            if(len(urlsetTag) > 0):\n",
        "              self.parsed_sitemap_domains.add(base_url)\n",
        "              sitemap_urls = [url.getText() for url in urlsetTag]\n",
        "              urls[0:0] = sitemap_urls\n",
        "            return urls\n",
        "        except:\n",
        "            print(f'Error in include_urls')\n",
        "        finally:\n",
        "            return urls\n",
        "\n",
        "    def crawler_url(self, url, q_size):\n",
        "        print(f'crawler: {url}')\n",
        "        urls = []\n",
        "        try:\n",
        "          base_url = self.get_base_url(url)\n",
        "\n",
        "          # Downloader\n",
        "          raw_html = self.get_raw_html(url)\n",
        "          if(raw_html == ''):\n",
        "            raise Exception(\"Empty page\")\n",
        "\n",
        "          # Analyzer\n",
        "          urls = self.link_parser(raw_html)\n",
        "          urls = self.include_urls(urls, base_url)\n",
        "          urls = self.normalization_urls(urls, base_url)\n",
        "          urls = self.filters_urls(urls, base_url)\n",
        "          urls = self.de_duplicate_urls(urls)\n",
        "\n",
        "          # store to disk\n",
        "          self.save_to_disk(url, raw_html)\n",
        "\n",
        "        except:\n",
        "          print(f'Error in crawler_url')\n",
        "        finally:\n",
        "          return urls\n",
        "\n",
        "\n",
        "    def get_current_i(self):\n",
        "        self.num_lock.acquire()\n",
        "        current_i = Crawler.current_i\n",
        "        Crawler.current_i += 1\n",
        "        self.num_lock.release()\n",
        "        return current_i\n",
        "\n",
        "    def run(self):\n",
        "        #--- main process ---#\n",
        "        current_i = self.get_current_i()\n",
        "\n",
        "        while(current_i < self.max_num):\n",
        "            self.url_lock.acquire()\n",
        "            q_size = self.frontier_q.qsize()\n",
        "            print(f\"\\nQueue Size: {q_size}\")\n",
        "            current_url = self.frontier_q.get()\n",
        "            time.sleep(self.sleep_time)\n",
        "            self.url_lock.release()\n",
        "\n",
        "            if(current_url is None):\n",
        "                continue\n",
        "\n",
        "            if current_url in self.visited:\n",
        "                continue\n",
        "            \n",
        "            print(f'i: {current_i}')\n",
        "            \n",
        "            try:\n",
        "                urls = self.crawler_url(current_url, q_size)\n",
        "                for url in urls:\n",
        "                    self.frontier_q.put(url)\n",
        "                print('\\n')\n",
        "                self.visited.add(current_url)\n",
        "\n",
        "                current_i = self.get_current_i()\n",
        "            except:\n",
        "                pass\n",
        "            finally:\n",
        "                pass\n",
        "                # self.frontier_q.task_done()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdU8L_pMwdIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417d9404-fd51-48e0-94c8-58ffbb4d9fa5"
      },
      "source": [
        "seed_url = 'https://ku.ac.th/th'\n",
        "\n",
        "number_of_threads = 64\n",
        "frontier_q = Queue()\n",
        "frontier_q.put(seed_url)\n",
        "visited = set()\n",
        "\n",
        "url_lock = threading.Lock()\n",
        "num_lock = threading.Lock()\n",
        "\n",
        "crawler_threads = []\n",
        "parsed_sitemap_domains = set()\n",
        "parsed_robots_domains = set()\n",
        "sleep_time = 0.1\n",
        "\n",
        "start_time = datetime.now()\n",
        "startTC('output.txt')\n",
        "\n",
        "for i in range(int(number_of_threads)):\n",
        "    crawler = Crawler(\n",
        "        sleep_time=sleep_time,\n",
        "        max_num=10000,\n",
        "        seed_url='https://ku.ac.th/th',\n",
        "        whitelist_file_types=['html', 'htm'],\n",
        "        whitelist_domain='ku.ac.th',\n",
        "        user_agent=\"Thitiwat_Bot\",\n",
        "        frontier_q=frontier_q,\n",
        "        visited=visited,\n",
        "        url_lock=url_lock,\n",
        "        num_lock=num_lock,\n",
        "        parsed_sitemap_domains=parsed_sitemap_domains,\n",
        "        parsed_robots_domains=parsed_robots_domains,\n",
        "    )\n",
        "    \n",
        "    crawler.start()\n",
        "    crawler_threads.append(crawler)\n",
        "\n",
        "\n",
        "for crawler in crawler_threads:\n",
        "    crawler.join()\n",
        "  \n",
        "time_elapsed = datetime.now() - start_time\n",
        "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
        "stopTC()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m2YpVu1okIK"
      },
      "source": [
        "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwlCiZujwdIj"
      },
      "source": [
        "def saveDomain(filename, urls):\n",
        "    urls = list(urls)\n",
        "    urls = [x for x in urls if x]\n",
        "    domains = []\n",
        "    for url in urls:\n",
        "        parsed = urlparse(url)\n",
        "        hostname = parsed.hostname\n",
        "        domain = re.sub(r'www.', '', hostname)\n",
        "        domains.append(domain)\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write('\\n'.join(domains))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aroVCgTAwdIj"
      },
      "source": [
        "saveDomain('list_robots.txt', list(parsed_robots_domains))\n",
        "saveDomain('list_sitemap.txt', list(parsed_sitemap_domains))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYRSSfdJbEAf",
        "outputId": "f38be255-62d0-4361-c35d-570252979fc0"
      },
      "source": [
        "parsed_robots_domains"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFV9kgfldTo3",
        "outputId": "7c12af58-f78b-4134-9249-e3d39972676d"
      },
      "source": [
        "parsed_sitemap_domains"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eSJGsyd6Uzl"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}