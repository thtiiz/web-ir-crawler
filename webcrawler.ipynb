{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "datamining-g-1",
   "display_name": "datamining-g-1",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import argparse\n",
    "import requests\n",
    "import os, codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from urllib.parse import urljoin, urlparse, unquote\n",
    "from requests.exceptions import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "\n",
    "    def __init__(self, seed_url, num_crawler, whitelist_file_types, user_agent, whitelist_domain):\n",
    "        self.seed_url = urljoin(seed_url, '/')[:-1]\n",
    "        self.frontier_q = [seed_url]\n",
    "        self.visited_q = []\n",
    "        self.num_crawler = num_crawler\n",
    "        self.whitelist_file_types = whitelist_file_types\n",
    "        self.user_agent = user_agent\n",
    "        self.whitelist_domain = whitelist_domain\n",
    "        self.headers = {\n",
    "            'User-Agent':  user_agent,\n",
    "            'From': 'thitiwat.tha@ku.th'\n",
    "        }\n",
    "        self.parsed_sitemap_domains = []\n",
    "        self.parsed_robots_domains = {}\n",
    "\n",
    "    # @param 'links' is a list of extracted links to be stored in the queue\n",
    "    def enqueue(self, links):\n",
    "        for link in links:\n",
    "            if link not in self.frontier_q and link not in self.visited_q:\n",
    "                self.frontier_q.append(link)\n",
    "\n",
    "    # FIFO queue\n",
    "    def dequeue(self):\n",
    "        current_url = self.frontier_q[0]\n",
    "        self.frontier_q = self.frontier_q[1:]\n",
    "        return current_url\n",
    "\n",
    "    def isQueueEmpty(self):\n",
    "        return len(self.frontier_q) == 0\n",
    "    \n",
    "    def saveDomain(self, filename, urls):\n",
    "        domains = []\n",
    "        for url in urls:\n",
    "            parsed = urlparse(url)\n",
    "            hostname = parsed.hostname\n",
    "            domain = re.sub(r'www.', '', hostname)\n",
    "            domains.append(domain)\n",
    "        domains = list(set(domains))\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write('\\n'.join(domains))\n",
    "\n",
    "    def link_parser(self, raw_html):\n",
    "        soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "        urls = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            urls.append(link.get('href'))\n",
    "        # pattern = '<a href=\"([^\"]*)\"'\n",
    "        # urls = re.findall(pattern, raw_html)\n",
    "        return urls\n",
    "\n",
    "    def de_duplicate_urls(self, urls):\n",
    "        return list(set(urls))\n",
    "    \n",
    "    def get_base_url(self, url):\n",
    "        return urljoin(url, '/')[:-1]\n",
    "\n",
    "    def save_to_disk(self, url, raw_html):\n",
    "        parsed = urlparse(url)\n",
    "        hostname = parsed.hostname\n",
    "        url_path = parsed.path\n",
    "\n",
    "        # print(f'hostname {hostname}')\n",
    "        save_folder_path = 'html/' + hostname + url_path\n",
    "        save_filename = 'index.html'\n",
    "        filetype = re.match(r'.*\\.(.*)$', url_path)\n",
    "\n",
    "        if(filetype == True):\n",
    "            # urljoin 'http://localhost:8888/asdasd/htasd.html' => 'http://localhost:8888/asdasd/'\n",
    "            save_filename = url.split(urljoin(url, '.'))[1]\n",
    "            save_abs_path = save_folder_path + save_filename\n",
    "        else:\n",
    "            save_abs_path = save_folder_path + '/' + save_filename\n",
    "\n",
    "        print(f'savepath: {save_folder_path}')\n",
    "        print(f'save filename: {save_filename}')\n",
    "        print(f'save_abs_path: {save_abs_path}')\n",
    "\n",
    "        os.makedirs(save_folder_path, 0o755, exist_ok=True)\n",
    "        f = codecs.open(save_abs_path, 'w', 'utf-8')\n",
    "        f.write(raw_html)\n",
    "        f.close()\n",
    "\n",
    "    def normalization_urls(self, urls, base_url):\n",
    "        # absolute\n",
    "        urls = [urljoin(base_url, url) for url in urls]\n",
    "\n",
    "        # remove # (self reference)\n",
    "        urls = [re.sub(r'#.*', '', url) for url in urls]\n",
    "\n",
    "        # parse to utf8\n",
    "        urls = [unquote(url) for url in urls]\n",
    "\n",
    "        # strip / (backslash)\n",
    "        urls = [url.strip('/') for url in urls]\n",
    "\n",
    "        return urls\n",
    "\n",
    "    def get_raw_html(self, url):\n",
    "        text = ''\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=20)\n",
    "            # If the response was successful, no Exception will be raised\n",
    "            response.raise_for_status()\n",
    "        except HTTPError as http_err:\n",
    "            print(f'HTTP error occurred: {http_err}')  # Python 3.6\n",
    "        except Exception as err:\n",
    "            print(f'Other error occurred: {err}')  # Python 3.6\n",
    "        else:\n",
    "            # print('Success!')\n",
    "            text = response.text\n",
    "        return text\n",
    "\n",
    "    def filters_urls(self, urls, base_url):\n",
    "        filtered_urls = []\n",
    "        if(base_url in self.parsed_robots_domains.keys()):\n",
    "            rp = self.parsed_robots_domains[base_url]\n",
    "        else:\n",
    "            rp = RobotFileParser()\n",
    "            rp.set_url(base_url + '/robots.txt')\n",
    "            rp.read()\n",
    "            self.parsed_robots_domains[base_url] = rp\n",
    "        \n",
    "        for url in urls:\n",
    "            parsed = urlparse(url)\n",
    "            url_path = parsed.path\n",
    "            hostname = parsed.hostname\n",
    "            # check domain allow only ku.ac.th\n",
    "            if(not hostname or self.whitelist_domain not in hostname):\n",
    "                continue\n",
    "\n",
    "            # check can fetch from robots.txt\n",
    "            can_fetch = rp.can_fetch(self.user_agent, url)\n",
    "            if(not can_fetch):\n",
    "                continue\n",
    "\n",
    "            # check filetype\n",
    "            filetype = re.match(r'.*\\.(.*)$', url_path)\n",
    "\n",
    "            if(not filetype):\n",
    "                filtered_urls.append(url)\n",
    "            elif(filetype[1] in self.whitelist_file_types):\n",
    "                filtered_urls.append(url)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return filtered_urls\n",
    "\n",
    "    def include_urls(self, urls, base_url):\n",
    "        if(base_url in self.parsed_sitemap_domains):\n",
    "            return urls\n",
    "        else:\n",
    "            self.parsed_sitemap_domains.append(base_url)\n",
    "\n",
    "        xml = self.get_raw_html(base_url + '/sitemap.xml')\n",
    "        soup = BeautifulSoup(xml)\n",
    "        urlsetTag = soup.find_all(\"loc\")\n",
    "        sitemap_urls = [url.getText() for url in urlsetTag]\n",
    "        urls[0:0] = sitemap_urls\n",
    "        return urls\n",
    "\n",
    "    def crawler_url(self, url):\n",
    "        print(f'crawler: {url}')\n",
    "        base_url = self.get_base_url(url)\n",
    "\n",
    "        # Downloader\n",
    "        raw_html = self.get_raw_html(url)\n",
    "\n",
    "        # Analyzer\n",
    "        urls = self.link_parser(raw_html)\n",
    "        urls = self.include_urls(urls, base_url)\n",
    "        urls = self.normalization_urls(urls, base_url)\n",
    "        urls = self.filters_urls(urls, base_url)\n",
    "        urls = self.de_duplicate_urls(urls)\n",
    "\n",
    "        # store to disk\n",
    "        self.save_to_disk(url, raw_html)\n",
    "        \n",
    "        return urls\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        #--- main process ---#\n",
    "        cur = 0\n",
    "        while(not self.isQueueEmpty() and cur < self.num_crawler):\n",
    "            print(f'i: {cur}')\n",
    "            current_url = self.dequeue()\n",
    "            self.visited_q.append(current_url)\n",
    "\n",
    "            urls = self.crawler_url(current_url)\n",
    "            self.enqueue(urls)\n",
    "            # print(urls)\n",
    "            cur += 1\n",
    "            print('\\n')\n",
    "            # print(self.frontier_q)\n",
    "            # print(self.visited_q)\n",
    "        self.saveDomain('list_robots.txt', self.parsed_robots_domains.keys())\n",
    "        self.saveDomain('list_sitemap.txt', self.parsed_sitemap_domains)\n",
    "        print(self.parsed_sitemap_domains)\n",
    "        print(self.parsed_robots_domains)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-n NUM]\nipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9054 --control=9052 --hb=9051 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"75006c2b-28c6-4b7a-bb3c-7b657d9370a5\" --shell=9053 --transport=\"tcp\" --iopub=9055 --f=/tmp/tmp-18114ZycH7Va1fjdY.json\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "2",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-n', '--num', help='number of crawlers', default=10000)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    num_crawler = int(args.num)\n",
    "    Scheduler(\n",
    "        seed_url ='https://ku.ac.th/th',\n",
    "        num_crawler = num_crawler,\n",
    "        whitelist_file_types = ['html', 'htm'],\n",
    "        whitelist_domain = 'ku.ac.th',\n",
    "        user_agent = \"Oporbot\"\n",
    "    ).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}